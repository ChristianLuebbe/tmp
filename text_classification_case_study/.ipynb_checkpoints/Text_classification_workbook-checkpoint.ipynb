{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Text classification :: Overview\n",
    "\n",
    "## Task \n",
    "\n",
    "We want to build a Spam detector which, given examples of spam emails (e.g. flagged by users) and examples of regular (non-spam, also called \"ham\") emails, learns how to flag new unseen emails as spam or non-spam.\n",
    "\n",
    "## Data\n",
    "\n",
    "We will use the [SpamAssassin](https://spamassassin.apache.org/) public email corpus. This dataset contains ~6'000 labeled emails with a ~30% spam ratio. If you want to learn more about this dataset, check [this](https://spamassassin.apache.org/old/publiccorpus/). (*Note: Datasets of text are called corpora and samples are called documents.*) \n",
    "\n",
    "The dataset has been downloaded for you and is available in the *data* folder.\n",
    "\n",
    "## Notebook overview\n",
    "\n",
    "* Load the data\n",
    "* Text preprocessing\n",
    "* Data exploration\n",
    "* Feature extraction\n",
    "* Build a spam detector\n",
    "* What did our model learn? Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification :: Spam detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries and helper functions\n",
    "import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = tools.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the number of samples per class in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.plot_class_frequency(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have a look at a few rows from the dataset.\n",
    "\n",
    "***Note:*** The *label* is 0 for *non-spam* and 1 for *spam*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you rerun this cell then you get a different set of samples displayed\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Text preprocessing\n",
    "\n",
    "Good text preprocessing is an essential part of every NLP project!\n",
    "\n",
    "Our goal here is to build a model that distinguishes non-spam from spam. The idea here is to \"clean\" and \"standardize\" raw text before feeding it to our machine learning model. We need to keep as many \"informative\" words as possible, while discarding the \"uniformative\" ones. Removing unnecessary content, i.e. the \"noise\", from our texts will help to improve the accuracy of our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3>Questions</h3>\n",
    "    \n",
    "Take a few minutes to look at the raw text.\n",
    "    \n",
    "__Q1.__ Which parts of the text do you think should be removed to make it readable?\n",
    "    \n",
    "__Q2.__ How could the parts that we have just removed for text cleaning still be useful for the distinction of spam and non-spam?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers\n",
    "\n",
    "__Q1.__ Here are some suggestions of what could be removed or changed in order to clean the text:\n",
    "\n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* ...\n",
    "\n",
    "__Q2.__ Your ideas:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *clean_corpus* function below will take care of the parts raised by Question 1.\n",
    "\n",
    "For the ideas from Question 2 we will create new features and investigate their effects in the subsection **What about \"spammish\" signatures?**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tools.clean_corpus(df)\n",
    "\n",
    "print(\"Data cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at a few \"cleaned\" examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.show_clean_text(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration :: What makes spam distinct?\n",
    "\n",
    "### Frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which words distinguish spam from non-spam? Can we  identify the words in a text that are the most informative about its topic?\n",
    "\n",
    "Let's find the 10 most frequent words in spam and non-spam and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.plot_most_common_words(df=df, N=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3>Questions</h3>\n",
    "    \n",
    "__Q1.__ Which of the top 10 most frequent \"spammish\" words are unique to that class?\n",
    "\n",
    "__Q2.__ Which of the top 10 most frequent \"non-spammish\" words are unique to that class?\n",
    "     \n",
    "__Q3.__ For the words that occur in both lists, which ones are likely still useful for distinguishing the classes?\n",
    "    \n",
    "__Q4.__ Which ones are likely not?\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers\n",
    "\n",
    "__Q1.__ **Frequent \"spammish\" words**: \n",
    "\n",
    "* \n",
    "* \n",
    "* \n",
    "\n",
    "\n",
    "__Q2.__ **Frequent \"non-spammish\" words**:\n",
    "* \n",
    "* \n",
    "* \n",
    "\n",
    "\n",
    "__Q3.__ **Occur in both top 10 but could be useful for distinctions**:\n",
    "* \n",
    "* \n",
    "* \n",
    "\n",
    "__Q4.__ **Occur in both top 10 but are unlikely to be useful**:\n",
    "* \n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3>Task 1</h3>\n",
    "    \n",
    "Change `N=10` to `N=30` and compare the outcome.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.plot_most_common_words(df=df, N=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "Add your obsevation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about \"spammish\" signatures?\n",
    "\n",
    "* Do spams contain more HTML tags? \n",
    "* Does non-spam contain more URLs and E-mail adresses? \n",
    "* Are spams mails longer than non-spam? \n",
    "* ...\n",
    "\n",
    "Let's find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tools.get_features(df=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering :: Extracting features from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computers don't understand natural language. So, how do we represent text?\n",
    "\n",
    "One of the simplest but effective and commonly used models to represent text for machine learning is the ***Bag of Words*** model ([online documentation](https://en.wikipedia.org/wiki/Bag-of-words_model)). When using this model, we discard most of the structure of the input text (word order, chapters, paragraphs, sentences and formating) and only count how often each word appears in each text. Discarding the structure and counting only word occurencies leads to the mental image of representing text as a \"bag\".  \n",
    "\n",
    "**Example:** Let our toy corpus contain four documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ corpus = ['I\\;enjoy\\;paragliding.',  $  \n",
    "$\\hspace{2cm}'I\\;like\\;NLP.',$  \n",
    "$\\hspace{2cm}'I\\;like\\;deep\\;learning.',$  \n",
    "$\\hspace{2cm}'O\\;Captain!\\;my\\;Captain!']$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.show_bag_of_words_vector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words has converted all documents into numeric vectors. Each column represents a word from the corpus and each row one of the four documents. The value in each cell represents the number of times that word appears in a specific document. For example, the fourth document has the word `captain` occuring twice and the words `my` and `O` occuring once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a spam detector\n",
    "\n",
    "In the previous section, we saw how to perform text preprocessing and feature extraction from text. We are now ready to build our machine learning model for detecting spams. We will use a Logistic Regression classifier ([online documentation](https://en.wikipedia.org/wiki/Logistic_regression)).\n",
    "\n",
    "First, we need to split the data into two sets: the `train` set and the `test` set. We will then use the train set to `fit` our model. The test set will be used to `evaluate` the performance of our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spam classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test splitting\n",
    "df_train, df_test = tools.train_test_split_(df)\n",
    "\n",
    "# Fit model on the train data\n",
    "model = tools.fit_model(df_train)\n",
    "\n",
    "# Print predictions on test set\n",
    "tools.plot_confusion_matrix(df_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion matrices**  \n",
    "\n",
    "Confusion matrices are a nice way of evaluating the performance of models for classification models. Rows correspond to the true classes and the columns to the predicted classes. Entries on the main diagonal of the confusion matrix correspond to correct predictions while the other cells tell us how many mistakes made our model ([online documentation]((https://en.wikipedia.org/wiki/Confusion_matrix))).\n",
    "\n",
    "* The first row represents non-spam mails: 1'187 were correctly classified as 'non-spam', while 29 (~2,3%) were misclassified as 'spam'.\n",
    "* The second row represents spam mails: 437 were correctly classified as 'spam', while 13 (~2,8%) were misclassified as 'non-spam'.\n",
    "\n",
    "Our model did quite well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What did our model learn from the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our logistic regression model has learned which words are the most indicative of non-spam and which words are the most indicative of spam. The positive coefficients on the right correspond to words that, according to the model, are indicative of spam. The negative coefficients on the left correspond to words that, according to the model, are indicative of non-spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.visualize_coefficients(model, n_top_features=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3>Questions</h3>\n",
    "    \n",
    "__Q1.__ According to the model which words are strong indicators of non-spam, respectively spam? \n",
    "    \n",
    "__Q2.__ Do they overlap with the results of our analysis?<br>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers\n",
    "\n",
    "__Q1.__ \n",
    "\n",
    "\n",
    "__Q2.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis :: Where does our model fail?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now analyze the misclassified mails in order to get some insights on where the model failed to make correct predictions. The *error_analysis* function below will show us the top features responsible for the model making a decision of prediction whether the mail is spam or non-spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.error_analysis(df_test, model, doc_nbr=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3>Task 2</h3>\n",
    "    \n",
    "Let's change the `doc_nbr`.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:text-workshop]",
   "language": "python",
   "name": "conda-env-text-workshop-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
